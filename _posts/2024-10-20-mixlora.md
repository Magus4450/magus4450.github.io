---
title: "Multimodal Instruction Tuning with Conditional Mixture of LoRA"
date: 2024-10-18
categories: [Research Papers]
tags: [LLM, LoRA, MoE, multimodal]
math: true
---


*Source: [Arxiv](https://arxiv.org/abs/2402.15896)*

---

## Background

If you haven't been living under a rock for the past few years, you should know what Large Language Models (LLMs) are and how they work. These seemingly complex models contain billions of trainable parameters (175 billion for GPT-4o) and require a lot of training time and data. The latest output of OpenAI is GPT-4o which is not only an LLM but a Multimodal Large Language Model (MLLM), meaning it can not only understand texts but also images and audios. These models are pre-trained on a large corpus of data first and are fine-tuned for specific tasks. The fine-tuning process is usually done by an algorithm called LoRA (Low Rank Adaptation of LLMs) which reduces the training cost, time and space. This paper argues that using LoRA or QLoRA (quantized version of LoRA) for multimodal task may bring forward issues to which they propose a new fine-tuning method, **MixLoRA**.

Before diving into the paper, let us understand how LLMs are trained and fine-tuned, and how LoRA works.

### LLM and Training

<img src="/assets/img/llm-training.png" style="width:600px;" alt="LLM Training">

The first step of training a LLM is called **pre-training**. This is the step that consumes the most time and resources. You take a very large corpus of data (basically a snapshot of the internet) and you train the model for task like prediction the next word in a sentence. This allows the model to learn how language works, learn about "this world" and so on. After pre-training the model is referred to as either **Base Model** or **Pre-trained Model**.

The next and the most important step in **fine-tuning**. This prepares the model to respond to instructions like ChatGPT. The dataset in this part is hand-crafted meticulously by humans. This dataset in then used to fine-tune the model by updating the weights very slowly.





### Finetuning Limitations

### LoRA

## Limitations of LoRA for MLLMs

## MixLoRA

### Interference

### Mixture of Experts (MoE)

### How it works

### Results

### Limitations





