---
title: "Multimodal Instruction Tuning with Conditional Mixture of LoRA"
date: 2024-11-25
categories: [Research Papers]
tags: [llm, lora, moe, multimodal, mixlora, fine-tuning]
math: true
---


*Source: [Arxiv](https://arxiv.org/abs/2402.15896)*

---

## Background

If you haven't been living under a rock for the past few years, you should know what Large Language Models (LLMs) are and how they work. These seemingly complex models contain billions of trainable parameters (175 billion for GPT-4o) and require a lot of training time and data. The latest output of OpenAI is GPT-4o which is not only an LLM but a Multimodal Large Language Model (MLLM), meaning it can not only understand texts but also images and audios. These models are pre-trained on a large corpus of data first and are fine-tuned for specific tasks. The fine-tuning process is usually done by an algorithm called LoRA (Low Rank Adaptation of LLMs) which reduces the training cost, time and space. This paper argues that using LoRA or QLoRA (quantized version of LoRA) for multimodal task may bring forward issues to which they propose a new fine-tuning method, **MixLoRA**.

Before diving into the paper, let us understand how LLMs are trained and fine-tuned, and how LoRA works.

### LLM and Training

<img src="/assets/img/llm-training.png" style="width:600px;" alt="LLM Training">

The first step of training a LLM is called **pre-training**. This is the step that consumes the most time and resources. You take a very large corpus of data (basically a snapshot of the internet) and you train the model for task like prediction the next word in a sentence. This allows the model to learn how language works, learn about "this world" and so on. After pre-training the model is referred to as either **Base Model** or **Pre-trained Model**.

The next and the most important step is **fine-tuning**. This prepares the model to respond to instructions like ChatGPT. The dataset in this part is hand-crafted meticulously by humans. This dataset in then used to fine-tune the model by updating the weights very slowly.



### Fine-tuning Limitations

Since LLMs have a lot of parameters, fine-tuning it means to update all of them every epoch. This is not only time consuming but also computationally expensive. Because of this, fine-tuning LLMs is not feasible for companies. To solve this, LoRA was introduced.

### LoRA


The authors of this paper argue that the updated weights after fine-tuning have very low "intrinsic rank" meaning the weight matrix can be represented by a much smaller matrix without much loss of information. So instead of learning a matrix of size $n \times m$, you can learn a matrix of size $n \times r$ and $r \times m$ where $r$ is much smaller than $n$ and $m$. The authors also argue that instead of fine-tuning the whole model, only some sections of the attention layer should be fine-tuned.

<img src="/assets/img/lora.png" style="width:600px;" alt="LORA">

The figure above represents one of those attention layers. $X$ is the input to the layer and $h$ is the output of the attention layer. While doing a forward pass, you combine the results of the pretrained model and the matrix multiplication of the two lower ranked matrices $A$ and $B$. $A$ matrix is initialized to a normal distribution whereas $B$ is initialized to 0. This makes the initial output of the layer to be the same as the output of the pretrained model. During the backward pass, the gradients are calculated for $A$ and $B$ and the weights are updated. The authors argue that this method is not only faster but also more efficient.

A very fascinating thing about LoRA is that you can train the model for different tasks and get different sets of $BA$. According to the type of task, you can choose the set of $BA$ that gives the best results.

## Limitations of LoRA for MLLMs

The authors argue that LoRA is not suitable for MLLMs because of the following reasons:

- **Different Modalities**: MLLMs have different modalities like text, image, audio, etc. LoRA is not designed to handle different modalities.
- **Swapping $BA$ takes time**


## MixLoRA

The authors of this paper introduce a novel approach, **MixLoRA** which is a conditional mixture of experts (MoE) based fine-tuning method. The idea is to have a set of $BA$ for each modality and then choose the best set of $BA$ based on the input. This process is handled by the model itself based on the input data.

### Interference

The idea of MixLoRA is based on the fact that there is change in performance of a model on task $i$ when fine-tuned for task $j$ and vice-versa. This is called **interference**. The authors have also formulated a way to quantify this interference. A value of 0 means no interference, a value greater than 0 means positive interference and a value less than 0 means negative interference.

<img src="/assets/img/task-interference.png" style="width:600px;" alt="Task Interference">

The figure above plots task-interference of different tasks. Each value is task interference for task $i$ when fine-tuned for task $j$. We can see a lot of values are significant further away from 0 meaning there is a lot of interference.


### Mixture of Experts (MoE)

<img src="/assets/img/moe.png" style="width:600px;" alt="MoE">


The idea of selection different set of $BA$ based on the input data is inspired by the MoE model. The MoE model is a model that has multiple sub-models and a gating network that decides which sub-model to use based on the input data. The main idea behind MoE is that instead of training one large model for different task, we train multiple smaller models (experts) and then use a gating network to decide which expert to use based on the input data. This performs significantly better than training one large model for all tasks.

### How it works

<img src="/assets/img/mixlora.png" style="width:600px;" alt="MixLoRA">

Let's understand the figure above one-by-one:

The multimodal input $h$ is first passed to the Independent Factor Selection (IFS) Router $R^A_{IFS}(.)$. The router selects $r$ different ranks from $f$ set of factors, concatenates them and creates the final $A$ matrix. For the selection of $B$, both the output from the Condition Factor Selection (CFS) Router and IFS Router is taken into account. After the selection of $A$ and $B$, the change is weights $\Delta W$ is calculated, and the weights are updated.


### Results

<img src="/assets/img/mixlora-res.png" style="width:600px;" alt="MixLoRA Results">

The figure above shows the results of MixLoRA on different multimodal tasks. The results show that MixLoRA outperforms LoRA on multiple different task and in some cases with even lower rank.

### Limitations

The major limitations highlighted in the paper are:

- The authors only implemented MixLoRA on image and text modalities but not 3D or audio.
- MixLoRA has extra training overhead compared to LoRA (because of MoE)
- MixLoRA doesn't use quantized version of LoRA (QLoRA) which is more efficient.




